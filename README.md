# ğŸ  House Price Prediction â€” v2 Enhanced ML Pipeline

This repository implements an **advanced, custom modular machine learning pipeline** for predicting median house prices in California. It represents the **second version (v2)** of the project, introducing engineered features, regularized linear models, cross-validation, hyperparameter tuning, and a custom Gradient Descent implementation built from scratch â€” all organized into reusable **source modules** powering the end-to-end training pipeline.

---

## ğŸ¯ Project Overview

The goal of this version is to:
- Build a **fully modular, extensible machine learning pipeline** for structured tabular data.
- Introduce **feature engineering and standardization** to improve model stability and performance.
- Implement and compare **multiple linear models**, including OLS, Ridge, Lasso, and a custom Gradient Descent Regressor built from scratch.
- Evaluate model robustness through **5-fold cross-validation** and targeted hyperparameter tuning.
- Establish a **reproducible training workflow** that cleanly separates preprocessing, model training, evaluation, and inference.

This version focuses on **engineering best practices**, enabling experimentation, modularity, and reliable performance benchmarking across models.

---

## ğŸ”„ Whatâ€™s New in v2 (Compared to v1)

A quick overview of improvements introduced in Version 2:

| Aspect | v1 Baseline | v2 Enhanced Pipeline |
|--------|-------------|----------------------|
| Feature Engineering | Minimal | Full transformations + scaling |
| Models | OLS only | GD (custom), OLS, Ridge, Lasso |
| Cross-Validation | No | Yes â€” 5-fold |
| Pipeline | Notebook-only | Modular Python pipeline (`src/`) |
| Performance | Higher RMSE | Lower RMSE after FE + regularization |

---

## âœ¨ Key Features

- **Custom Modular ML pipeline** (`src/` folder) for clean separation of preprocessing, training, evaluation, and utilities.
- **Feature engineering & standardization**, ensuring consistent transformations across training and inference.
- **Multiple linear models**:
  - OLS (Linear Regression)
  - Ridge Regression (L2)
  - Lasso Regression (L1)
- **Custom Gradient Descent Regressor** implemented from scratch with configurable learning rate, iterations, and convergence tracking.
- **5-fold Cross-Validation** for assessing model stability and variance.
- **Hyperparameter tuning** for regularized models using a simple grid search workflow.
- **Reproducible training pipeline** (`python3 -m src.train`) for end-to-end training and evaluation.
- **Five structured Jupyter notebooks** documenting the full development process.

---

## ğŸ§± Repository Structure
```
house-price-ml-v2/
â”‚
â”œâ”€â”€ data/                                  # Local dataset storage (not included in Git intentionally for privacy / size)
â”‚   â”œâ”€â”€ processed/                         # Cleaned / transformed data (not saved in this project; kept ephemeral)
â”‚   â””â”€â”€ raw/                               # Unmodified input data (as downloaded)
â”‚   
â”œâ”€â”€ models/                                # Saved model artifacts (not included in Git to avoid large files and ensure reproducible training)
â”‚
â”œâ”€â”€ notebooks/                             # Full development workflow (v2 notebooks)
â”‚   â”œâ”€â”€ 01_exploration.ipynb               # Initial EDA, data inspection, distributions, correlations
â”‚   â”œâ”€â”€ 02_model_evaluation.ipynb          # Training and evaluating the custom Gradient Descent Regressor
â”‚   â”œâ”€â”€ 03_sklearn_baseline.ipynb          # Baseline Linear Regression using scikit-learn (LinearRegression)
â”‚   â”œâ”€â”€ 04_cross_validation.ipynb          # 5-fold CV comparing OLS, Ridge, and Lasso models, stability analysis
â”‚   â””â”€â”€ 05_pipeline_demo.ipynb             # End-to-end demonstration of the modular training pipeline
â”‚
â”œâ”€â”€ reports/                               # Project documentation and reports
â”‚   â”œâ”€â”€ metrics/
â”‚   â”œâ”€â”€ plots/
â”‚   â””â”€â”€ report.md                          # Detailed technical write-up for Version 2
â”‚
â”œâ”€â”€ src/                                   # Modular machine learning pipeline (from scratch)
â”‚   â”œâ”€â”€ __init__.py                        # Marks directory as a Python package
â”‚   â”œâ”€â”€ config.py                          # Centralized configuration / constants
â”‚   â”œâ”€â”€ data_loader.py                     # Data loading utilities for datasets
â”‚   â”œâ”€â”€ evaluation.py                      # Metrics, scoring utilities, model evaluation logic
â”‚   â”œâ”€â”€ feature_engineering.py             # Data cleaning, feature transformations, scaling logic
â”‚   â”œâ”€â”€ gradient_descent.py                # Custom Gradient Descent Regressor
â”‚   â”œâ”€â”€ hyperparameter_tuning.py           # Grid search utilities
â”‚   â”œâ”€â”€ model_io.py                        # Save/load functions for pipeline components and artifacts
â”‚   â”œâ”€â”€ preprocessing.py                   # Preprocessing utilities (standardization)
â”‚   â””â”€â”€ train.py                           # End-to-end pipeline (run via: python3 -m src.train)
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

> ğŸ—’ï¸ **Note:**  
> Version 2 uses a **fully modular architecture** inside `src/`, and the `notebooks/` directory follows a clean, sequential workflow from exploration â†’ baselines â†’ cross-validation â†’ final pipeline.
> - The `data/` directory is **not tracked by Git** to avoid storing large files and to keep the repository lightweight. Only *raw* data should be placed here. The `data/processed/` folder exists as a placeholder for a scalable workflow, but **no processed data is saved in this project** â€” all transformations are generated dynamically by the pipeline / notebooks.  
> - The `models/` directory is also **excluded from Git**, since model artifacts are generated during training and can be reproduced at any time by running the pipeline.

---

## ğŸ§° Run Locally

You can run this project on your machine using **Python 3.11** and `venv`.

### 1ï¸âƒ£ Clone the repository
```bash
git clone git@github.com:florykhan/house-price-ml-v2.git
cd house-price-ml-v2
```

### 2ï¸âƒ£ Create and activate a virtual environment
```bash
python3 -m venv venv
source venv/bin/activate      # macOS/Linux
venv\Scripts\activate         # Windows
```
### 3ï¸âƒ£ Install dependencies
```bash
pip3 install -r requirements.txt
```

### 4ï¸âƒ£ Add the dataset
Place `housing.csv` inside the `data/raw/` folder:
```bash
house-price-ml-v2/data/raw/housing.csv
```
> ğŸ“¥ **Download the dataset:**  
> - Via scikit-learn:  
>   ```python
>   from sklearn.datasets import fetch_california_housing
>   data = fetch_california_housing(as_frame=True)
>   ```
> - Or download the CSV from [Kaggle](https://www.kaggle.com/datasets/camnugent/california-housing-prices)
>
> âš ï¸ **Note:**  Processed data is not saved in this project. All transformations are applied dynamically through the pipeline.

### 5ï¸âƒ£ Run the training pipeline
This step is essential â€” the training pipeline performs all preprocessing, feature engineering, and model training needed for the notebooks to run correctly.

```bash
python3 -m src.train
```

### 6ï¸âƒ£ Run the notebooks
Launch Jupyter and open the notebooks:
```bash
jupyter notebook
```

Recommended order:
- `01_exploration.ipynb` â€” data exploration
- `02_model_evaluation.ipynb` â€” custom Gradient Descent
- `03_sklearn_baseline.ipynb` â€” sklearn LinearRegression
- `04_cross_validation.ipynb` â€” OLS/Ridge/Lasso CV comparison
- `05_pipeline_demo.ipynb` â€” full end-to-end pipeline demo

---

## ğŸ“Š Results (Summary)

| **Custom Gradient Descent Regressor** | **Interpretation** |
|---------------------------------------|---------------------|
| â€¢ Converged in ~1500 iterations<br>â€¢ Test RMSE: **~74.6K USD**<br>â€¢ Test RÂ²: **~0.57** | â€¢ Explains **~57%** of variance in housing prices.<br>â€¢ Captures strong **linear trends** (e.g., median income â†’ price).<br>â€¢ Misses **nonlinear** and **interaction** effects â†’ room for improvement. |



â¡ï¸ For full model comparisons (Ridge, Lasso, CV results, etc.), see the full report: [`reports/report.md`](reports/report.md)

---

## ğŸ“„ Full Technical Report

For the complete technical write-up â€” including model comparisons, cross-validation results, feature engineering details, and a full discussion of Version 2 improvements â€” see: [`reports/report.md`](reports/report.md). This document contains all deep-level explanations intended for reviewers who want to understand the full methodology behind the pipeline, models, experiments, and results.

---

9. Future Work
in the next project combine hyperparameter tuning with cv ğŸ”¥
---

## ğŸ§  Tech Stack

- **Language:** Python 3.11  
- **Core Libraries:**  
  - `pandas`, `numpy`, `matplotlib`  
  - `scikit-learn`
- **Custom Components:**  
  - Custom Gradient Descent Regressor  
  - Reusable feature engineering, evaluation, and tuning modules (`src/` folder)
  - End-to-end training pipeline (`python3 -m src.train`)
- **Environment:** Jupyter Notebook / VS Code
- **Version Control:** Git + GitHub (SSH configured)

---

## ğŸ§¾ License
MIT License â€” feel free to use and modify with attribution.
See the [`LICENSE`](./LICENSE) file for full details.

---

## ğŸ‘¤ Author
**Ilian Khankhalaev**  
_BSc Computing Science, Simon Fraser University_  
ğŸ“ Vancouver, BC  |  [florykhan@gmail.com](mailto:florykhan@gmail.com)  |  [GitHub](https://github.com/florykhan)  |  [LinkedIn](https://www.linkedin.com/in/ilian-khankhalaev/)
